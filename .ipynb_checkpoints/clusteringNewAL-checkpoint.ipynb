{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "offline.init_notebook_mode(connected=True)\n",
    "from discreteMarkovChain import markovChain\n",
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "#apply code Sarafina wrote to Hector's dataset\n",
    "#run source sink code on those clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('ArtificialLandscapes/data/unzipForMaps/R030_P050_C003_D010_CMgM.csv', header=None)\n",
    "\n",
    "\n",
    "npmat = np.matrix(test)\n",
    "test = nx.from_numpy_matrix(npmat, create_using = nx.DiGraph())\n",
    "#nx.write_pajek(test, \"test.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Temperature        Energy    Accept   Improve     Elapsed   Remaining\n",
      " 22800.27098          8.90   100.00%    36.00%     0:00:00     0:00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a graph (20 nodes, 400 edges)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     2.50000          8.89   100.00%    27.00%     0:00:06     0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 mile route:\n",
      "\t [8, 2, 4, 11, 1, 5, 6, 19, 15, 0, 17, 3]\n",
      "\t [16, 9, 14, 7]\n",
      "\t [13, 10, 18, 12]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import random\n",
    "import simanneal\n",
    "from simanneal import Annealer\n",
    "\n",
    "import argparse\n",
    "from math import log\n",
    "import networkx as nx\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import hamming\n",
    "# def distance(a, b):\n",
    "#     \"\"\"Calculates distance between two latitude-longitude coordinates.\"\"\"\n",
    "#     R = 3963  # radius of Earth (miles)\n",
    "#     lat1, lon1 = math.radians(a[0]), math.radians(a[1])\n",
    "#     lat2, lon2 = math.radians(b[0]), math.radians(b[1])\n",
    "#     return math.acos(math.sin(lat1) * math.sin(lat2) +\n",
    "#                      math.cos(lat1) * math.cos(lat2) * math.cos(lon1 - lon2)) * R\n",
    "\n",
    "TAU = 0.15\n",
    "PAGE_RANK = 'page_rank'\n",
    "MODULE_ID = 'module_id'\n",
    "\n",
    "def log2(prob):\n",
    "    \"Returns the log of prob in base 2\"\n",
    "    return log(prob, 2)\n",
    "\n",
    "def entropy1(prob):\n",
    "    \"\"\"Half of the entropy function, as used in the InfoMap paper.\n",
    "    entropy1(p) = p * log2(p)\n",
    "    \"\"\"\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    return prob * log2(prob)\n",
    "\n",
    "def load_and_process_graph(filename):\n",
    "    \"\"\"Load the graph, normalize edge weights, compute pagerank, and store all\n",
    "    this back in node data.\"\"\"\n",
    "    # Load the graph\n",
    "    graph = nx.DiGraph(nx.read_pajek(filename))\n",
    "    #graph = nx.read_pajek(filename)\n",
    "    print (\"Loaded a graph (%d nodes, %d edges)\" % (len(graph),\n",
    "            len(graph.edges())))\n",
    "    # Compute the normalized edge weights\n",
    "    for node in graph:\n",
    "        edges = graph.edges(node, data=True)\n",
    "        total_weight = sum([data['weight'] for (_, _, data) in edges])\n",
    "        for (_, _, data) in edges:\n",
    "            data['weight'] = data['weight'] / total_weight\n",
    "    # Get its PageRank, alpha is 1-tau where [RAB2009 says \\tau=0.15]\n",
    "    page_ranks = nx.pagerank(graph, alpha=1-TAU)\n",
    "    for (node, page_rank) in page_ranks.items():\n",
    "        graph.node[node][PAGE_RANK] = page_rank\n",
    "    return graph\n",
    "\n",
    "def load_coordinates(filename):\n",
    "    field_names = ['X', 'Y', \"w\"]\n",
    "    coords = pd.read_csv(filename, header=None, names=field_names)\n",
    "    coords = coords.loc[:,[\"X\",\"Y\"]]\n",
    "    #coords = coords.as_matrix()\n",
    "    return coords\n",
    "\n",
    "class GeoInfomap(Annealer):\n",
    "\n",
    "    \"\"\"Test annealer with a travelling salesman problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # pass extra data (the distance matrix) into the constructor\n",
    "    def __init__(self, state, module, graph, coordinates):\n",
    "        self.graph = graph\n",
    "        self.total_pr_entropy = sum([entropy1(graph.node[node][PAGE_RANK]) \\\n",
    "                for node in graph])\n",
    "#         self.module = [Module(module_id, mod, graph) \\\n",
    "#                 for (module_id, mod) in enumerate(state)]\n",
    "        d = 0\n",
    "        for mod in module:\n",
    "            for elem in range(len(mod)):\n",
    "                mod[elem] = int(mod[elem])    \n",
    "        for mod in module:\n",
    "            m = coordinates.loc[mod,]\n",
    "            d += np.mean(pairwise_distances(m, metric='euclidean'))\n",
    "        self.d = d \n",
    "\n",
    "        super(GeoInfomap, self).__init__(state)  # important!\n",
    "    def move(self):\n",
    "        #converts list of node lists into a 1D array of community labels\n",
    "        cluster_labels = []\n",
    "        label = 0\n",
    "        for cluster in self.state:\n",
    "            for elem in cluster:\n",
    "                cluster_labels.append(label)\n",
    "            label += 1\n",
    "\n",
    "    #flatten self.state\n",
    "        flat_list = [item for sublist in self.state for item in sublist]\n",
    "        #sort cluster labels list by node label (not community label)\n",
    "        cluster_labels = [cluster_labels[flat_list.index(i)] for i in flat_list]\n",
    "        a = random.randint(0, len(cluster_labels)-1)\n",
    "        change_node = cluster_labels[a] \n",
    "        #if current label is 0, change to 1 to increase hamming distance by 1\n",
    "        if change_node == 0:\n",
    "            cluster_labels[a] = 1\n",
    "        else:\n",
    "            updown = random.randint(0, 1)\n",
    "            if updown == 0:\n",
    "                cluster_labels[a] -= 1\n",
    "            cluster_labels[a] += 1\n",
    "\n",
    "        #convert back to list of lists\n",
    "        new_state = []\n",
    "        labels = set(cluster_labels)\n",
    "        for j in labels:\n",
    "            cluster = []\n",
    "            indices = [i for i, x in enumerate(cluster_labels) if x == j]\n",
    "            cluster.extend(list(np.array(flat_list)[indices]))\n",
    "            new_state.append(cluster)\n",
    "\n",
    "        self.state = new_state\n",
    "\n",
    "    def energy(self):\n",
    "        \"Compute the MDL of this clustering according to [RAB2009, eq. 4]\"\n",
    "        graph = self.graph\n",
    "        \n",
    "        total_qout = 0\n",
    "        total_qout_entropy = 0\n",
    "        total_both_entropy = 0\n",
    "        for mod in self.state:\n",
    "            nodes = frozenset(mod)\n",
    "            prop_nodes = 1 - float(len(nodes)) / len(graph)\n",
    "            total_pr = sum([graph.node[str(node)][PAGE_RANK] for node in nodes])\n",
    "            q_out = total_pr * TAU * prop_nodes\n",
    "\n",
    "            for node in mod:\n",
    "                edges = graph.edges(str(node), data=True)\n",
    "                page_rank = graph.node[str(node)][PAGE_RANK]\n",
    "                if len(edges) == 0:\n",
    "                    q_out += page_rank * prop_nodes * (1 - TAU)\n",
    "                    continue\n",
    "                for (_, dest, data) in edges:\n",
    "                    if dest not in self.state:\n",
    "                        q_out += page_rank * data['weight'] * (1 - TAU)\n",
    "                q_plus_p = q_out + total_pr\n",
    "        \n",
    "            q_out = q_out\n",
    "            total_qout += q_out\n",
    "            total_qout_entropy += entropy1(q_out)\n",
    "            total_both_entropy += entropy1(q_plus_p)\n",
    "        term1 = entropy1(total_qout)\n",
    "        term2 = -2 * total_qout_entropy\n",
    "        term3 = -self.total_pr_entropy\n",
    "        term4 = total_both_entropy\n",
    "        term5 = self.d\n",
    "        total =  term1 + term2 + term3 + term4 + 0.1*term5\n",
    "        #print(total)\n",
    "        return total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #networkX Digraph\n",
    "    graph = load_and_process_graph(\"houses.net\")#(options.graph_filename)\n",
    "\n",
    "    #coords is pandas dataframe of the coordinates\n",
    "    coords = load_coordinates(\"coordinates.csv\")\n",
    "    coords.index = np.arange(0, len(coords))\n",
    "\n",
    "    # single_nodes is the \"trivial\" module mapping\n",
    "    # initial module, as a list of lists\n",
    "    single_nodes = [[nodes] for nodes in graph]\n",
    "    single_nodes[0] = ['0','1']\n",
    "    single_nodes.remove(single_nodes[1]) \n",
    "    init_state = single_nodes\n",
    "\n",
    "    gi = GeoInfomap(state = init_state, module = init_state, graph = graph, coordinates = coords)\n",
    "    gi.steps = 10000\n",
    "    # since our state is just a list, slice is the fastest way to copy\n",
    "    gi.copy_strategy = \"slice\"\n",
    "    state, e = gi.anneal()\n",
    "        \n",
    "    print()\n",
    "   # print(state)\n",
    "    print(\"%i mile route:\" % e)\n",
    "    for city in state:\n",
    "        print(\"\\t\", city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
