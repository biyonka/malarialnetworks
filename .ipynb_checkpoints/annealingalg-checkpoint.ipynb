{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Temperature        Energy    Accept   Improve     Elapsed   Remaining\n",
      " 20794.09428          8.89   100.00%    23.00%     0:00:00     0:00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a graph (20 nodes, 400 edges)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     2.50000          8.86   100.00%    26.00%     0:00:05     0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 mile route:\n",
      "\t [7, 1, 2, 6, 8, 17]\n",
      "\t [10, 0, 15, 3, 12, 4, 11, 16, 18, 14, 5]\n",
      "\t [9, 19, 13]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import random\n",
    "import simanneal\n",
    "from simanneal import Annealer\n",
    "\n",
    "import argparse\n",
    "from math import log\n",
    "import networkx as nx\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import hamming\n",
    "# def distance(a, b):\n",
    "#     \"\"\"Calculates distance between two latitude-longitude coordinates.\"\"\"\n",
    "#     R = 3963  # radius of Earth (miles)\n",
    "#     lat1, lon1 = math.radians(a[0]), math.radians(a[1])\n",
    "#     lat2, lon2 = math.radians(b[0]), math.radians(b[1])\n",
    "#     return math.acos(math.sin(lat1) * math.sin(lat2) +\n",
    "#                      math.cos(lat1) * math.cos(lat2) * math.cos(lon1 - lon2)) * R\n",
    "\n",
    "TAU = 0.15\n",
    "PAGE_RANK = 'page_rank'\n",
    "MODULE_ID = 'module_id'\n",
    "\n",
    "def log2(prob):\n",
    "    \"Returns the log of prob in base 2\"\n",
    "    return log(prob, 2)\n",
    "\n",
    "def entropy1(prob):\n",
    "    \"\"\"Half of the entropy function, as used in the InfoMap paper.\n",
    "    entropy1(p) = p * log2(p)\n",
    "    \"\"\"\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    return prob * log2(prob)\n",
    "\n",
    "def load_and_process_graph(filename):\n",
    "    \"\"\"Load the graph, normalize edge weights, compute pagerank, and store all\n",
    "    this back in node data.\"\"\"\n",
    "    # Load the graph\n",
    "    graph = nx.DiGraph(nx.read_pajek(filename))\n",
    "    #graph = nx.read_pajek(filename)\n",
    "    print (\"Loaded a graph (%d nodes, %d edges)\" % (len(graph),\n",
    "            len(graph.edges())))\n",
    "    # Compute the normalized edge weights\n",
    "    for node in graph:\n",
    "        edges = graph.edges(node, data=True)\n",
    "        total_weight = sum([data['weight'] for (_, _, data) in edges])\n",
    "        for (_, _, data) in edges:\n",
    "            data['weight'] = data['weight'] / total_weight\n",
    "    # Get its PageRank, alpha is 1-tau where [RAB2009 says \\tau=0.15]\n",
    "    page_ranks = nx.pagerank(graph, alpha=1-TAU)\n",
    "    for (node, page_rank) in page_ranks.items():\n",
    "        graph.node[node][PAGE_RANK] = page_rank\n",
    "    return graph\n",
    "\n",
    "def load_coordinates(filename):\n",
    "    field_names = ['X', 'Y', \"w\"]\n",
    "    coords = pd.read_csv(filename, header=None, names=field_names)\n",
    "    coords = coords.loc[:,[\"X\",\"Y\"]]\n",
    "    #coords = coords.as_matrix()\n",
    "    return coords\n",
    "\n",
    "# class Module:\n",
    "#     \"\"\"Stores the information about a single module\"\"\"\n",
    "#     def __init__(self, module_id, nodes, graph):\n",
    "#         self.module_id = module_id\n",
    "#         self.nodes = frozenset(nodes)\n",
    "#         self.graph = graph\n",
    "#         self.prop_nodes = 1 - float(len(self.nodes)) / len(graph)\n",
    "#         # Set the module_id for every node\n",
    "# #         for node in nodes:\n",
    "# #             graph.node[node][MODULE_ID] = module_id\n",
    "#         # Compute the total PageRank\n",
    "#         self.total_pr = sum([graph.node[node][PAGE_RANK] for node in nodes])\n",
    "#         # Compute q_out, the exit probability of this module\n",
    "#         # .. Left half: tau * (n - n_i) / n * sum{alpha in i}(p_alpha)\n",
    "#         self.q_out = self.total_pr * TAU * self.prop_nodes\n",
    "#         # .. Right half: (1-tau) * sum{alpha in i}(sum{beta not in i}\n",
    "#         #                  p_alpha weight_alpha,beta)\n",
    "#         # This is what's in [RAB2009 eq. 6]. But it's apparently wrong if\n",
    "#         # node alpha has no out-edges, which is not in the paper.\n",
    "#         # ..\n",
    "#         # Implementing it with Seung-Hee's correction about dangling nodes\n",
    "#         for node in self.nodes:\n",
    "#             edges = graph.edges(node, data=True)\n",
    "#             page_rank = graph.node[node][PAGE_RANK]\n",
    "#             if len(edges) == 0:\n",
    "#                 self.q_out += page_rank * self.prop_nodes * (1 - TAU)\n",
    "#                 continue\n",
    "#             for (_, dest, data) in edges:\n",
    "#                 if dest not in self.nodes:\n",
    "#                     self.q_out += page_rank * data['weight'] * (1 - TAU)\n",
    "#         self.q_plus_p = self.q_out + self.total_pr\n",
    "\n",
    "#     def get_codebook_length(self):\n",
    "#         \"Computes module codebook length according to [RAB2009, eq. 3]\"\n",
    "#         first = -entropy1(self.q_out / self.q_plus_p)\n",
    "#         second = -sum( \\\n",
    "#                 [entropy1(self.graph.node[node][PAGE_RANK]/self.q_plus_p) \\\n",
    "#                     for node in self.nodes])\n",
    "#         return (self.q_plus_p) * (first + second)\n",
    "\n",
    "class GeoInfomap(Annealer):\n",
    "\n",
    "    \"\"\"Test annealer with a travelling salesman problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # pass extra data (the distance matrix) into the constructor\n",
    "    def __init__(self, state, module, graph, coordinates):\n",
    "        self.graph = graph\n",
    "        self.total_pr_entropy = sum([entropy1(graph.node[node][PAGE_RANK]) \\\n",
    "                for node in graph])\n",
    "#         self.module = [Module(module_id, mod, graph) \\\n",
    "#                 for (module_id, mod) in enumerate(state)]\n",
    "        d = 0\n",
    "        for mod in module:\n",
    "            for elem in range(len(mod)):\n",
    "                mod[elem] = int(mod[elem])    \n",
    "        for mod in module:\n",
    "            m = coordinates.loc[mod,]\n",
    "            d += np.mean(pairwise_distances(m, metric='euclidean'))\n",
    "        self.d = d \n",
    "\n",
    "        super(GeoInfomap, self).__init__(state)  # important!\n",
    "    def move(self):\n",
    "        #converts list of node lists into a 1D array of community labels\n",
    "        cluster_labels = []\n",
    "        label = 0\n",
    "        for cluster in self.state:\n",
    "            for elem in cluster:\n",
    "                cluster_labels.append(label)\n",
    "            label += 1\n",
    "\n",
    "    #flatten self.state\n",
    "        flat_list = [item for sublist in self.state for item in sublist]\n",
    "        #sort cluster labels list by node label (not community label)\n",
    "        cluster_labels = [cluster_labels[flat_list.index(i)] for i in flat_list]\n",
    "        a = random.randint(0, len(cluster_labels)-1)\n",
    "        change_node = cluster_labels[a] \n",
    "        #if current label is 0, change to 1 to increase hamming distance by 1\n",
    "        if change_node == 0:\n",
    "            cluster_labels[a] = 1\n",
    "        else:\n",
    "            updown = random.randint(0, 1)\n",
    "            if updown == 0:\n",
    "                cluster_labels[a] -= 1\n",
    "            cluster_labels[a] += 1\n",
    "\n",
    "        #convert back to list of lists\n",
    "        new_state = []\n",
    "        labels = set(cluster_labels)\n",
    "        for j in labels:\n",
    "            cluster = []\n",
    "            indices = [i for i, x in enumerate(cluster_labels) if x == j]\n",
    "            cluster.extend(list(np.array(flat_list)[indices]))\n",
    "            new_state.append(cluster)\n",
    "\n",
    "        self.state = new_state\n",
    "\n",
    "    def energy(self):\n",
    "        \"Compute the MDL of this clustering according to [RAB2009, eq. 4]\"\n",
    "        graph = self.graph\n",
    "        \n",
    "        total_qout = 0\n",
    "        total_qout_entropy = 0\n",
    "        total_both_entropy = 0\n",
    "        for mod in self.state:\n",
    "            nodes = frozenset(mod)\n",
    "            prop_nodes = 1 - float(len(nodes)) / len(graph)\n",
    "            total_pr = sum([graph.node[str(node)][PAGE_RANK] for node in nodes])\n",
    "            q_out = total_pr * TAU * prop_nodes\n",
    "\n",
    "            for node in mod:\n",
    "                edges = graph.edges(str(node), data=True)\n",
    "                page_rank = graph.node[str(node)][PAGE_RANK]\n",
    "                if len(edges) == 0:\n",
    "                    q_out += page_rank * prop_nodes * (1 - TAU)\n",
    "                    continue\n",
    "                for (_, dest, data) in edges:\n",
    "                    if dest not in self.state:\n",
    "                        q_out += page_rank * data['weight'] * (1 - TAU)\n",
    "                q_plus_p = q_out + total_pr\n",
    "        \n",
    "            q_out = q_out\n",
    "            total_qout += q_out\n",
    "            total_qout_entropy += entropy1(q_out)\n",
    "            total_both_entropy += entropy1(q_plus_p)\n",
    "        term1 = entropy1(total_qout)\n",
    "        term2 = -2 * total_qout_entropy\n",
    "        term3 = -self.total_pr_entropy\n",
    "        term4 = total_both_entropy\n",
    "        term5 = self.d\n",
    "        total =  term1 + term2 + term3 + term4 + 0.1*term5\n",
    "        #print(total)\n",
    "        return total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #networkX Digraph\n",
    "    graph = load_and_process_graph(\"houses.net\")#(options.graph_filename)\n",
    "\n",
    "    #coords is pandas dataframe of the coordinates\n",
    "    coords = load_coordinates(\"coordinates.csv\")\n",
    "    coords.index = np.arange(0, len(coords))\n",
    "\n",
    "    # single_nodes is the \"trivial\" module mapping\n",
    "    # initial module, as a list of lists\n",
    "    single_nodes = [[nodes] for nodes in graph]\n",
    "    single_nodes[0] = ['0','1']\n",
    "    single_nodes.remove(single_nodes[1]) \n",
    "    init_state = single_nodes\n",
    "\n",
    "    gi = GeoInfomap(state = init_state, module = init_state, graph = graph, coordinates = coords)\n",
    "    gi.steps = 10000\n",
    "    # since our state is just a list, slice is the fastest way to copy\n",
    "    gi.copy_strategy = \"slice\"\n",
    "    state, e = gi.anneal()\n",
    "        \n",
    "    print()\n",
    "   # print(state)\n",
    "    print(\"%i mile route:\" % e)\n",
    "    for city in state:\n",
    "        print(\"\\t\", city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Temperature        Energy    Accept   Improve     Elapsed   Remaining\n",
      "     2.50000       6898.57     6.00%     0.10%     0:00:01     0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6882 mile route:\n",
      "\t New York City\n",
      "\t Columbus\n",
      "\t Detroit\n",
      "\t Chicago\n",
      "\t Indianapolis\n",
      "\t Memphis\n",
      "\t Dallas\n",
      "\t Fort Worth\n",
      "\t Phoenix\n",
      "\t San Francisco\n",
      "\t San Jose\n",
      "\t Los Angeles\n",
      "\t San Diego\n",
      "\t San Antonio\n",
      "\t Austin\n",
      "\t Houston\n",
      "\t Jacksonville\n",
      "\t Charlotte\n",
      "\t Baltimore\n",
      "\t Philadelphia\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import random\n",
    "from simanneal import Annealer\n",
    "\n",
    "\n",
    "def distance(a, b):\n",
    "    \"\"\"Calculates distance between two latitude-longitude coordinates.\"\"\"\n",
    "    R = 3963  # radius of Earth (miles)\n",
    "    lat1, lon1 = math.radians(a[0]), math.radians(a[1])\n",
    "    lat2, lon2 = math.radians(b[0]), math.radians(b[1])\n",
    "    return math.acos(math.sin(lat1) * math.sin(lat2) +\n",
    "                     math.cos(lat1) * math.cos(lat2) * math.cos(lon1 - lon2)) * R\n",
    "\n",
    "\n",
    "class TravellingSalesmanProblem(Annealer):\n",
    "\n",
    "    \"\"\"Test annealer with a travelling salesman problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # pass extra data (the distance matrix) into the constructor\n",
    "    def __init__(self, state, distance_matrix):\n",
    "        self.distance_matrix = distance_matrix\n",
    "        super(TravellingSalesmanProblem, self).__init__(state)  # important!\n",
    "\n",
    "    def move(self):\n",
    "        \"\"\"Swaps two cities in the route.\"\"\"\n",
    "        a = random.randint(0, len(self.state) - 1)\n",
    "        b = random.randint(0, len(self.state) - 1)\n",
    "        self.state[a], self.state[b] = self.state[b], self.state[a]\n",
    "\n",
    "    def energy(self):\n",
    "        \"\"\"Calculates the length of the route.\"\"\"\n",
    "        e = 0\n",
    "        for i in range(len(self.state)):\n",
    "            e += self.distance_matrix[self.state[i-1]][self.state[i]]\n",
    "        return e\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # latitude and longitude for the twenty largest U.S. cities\n",
    "    cities = {\n",
    "        'New York City': (40.72, 74.00),\n",
    "        'Los Angeles': (34.05, 118.25),\n",
    "        'Chicago': (41.88, 87.63),\n",
    "        'Houston': (29.77, 95.38),\n",
    "        'Phoenix': (33.45, 112.07),\n",
    "        'Philadelphia': (39.95, 75.17),\n",
    "        'San Antonio': (29.53, 98.47),\n",
    "        'Dallas': (32.78, 96.80),\n",
    "        'San Diego': (32.78, 117.15),\n",
    "        'San Jose': (37.30, 121.87),\n",
    "        'Detroit': (42.33, 83.05),\n",
    "        'San Francisco': (37.78, 122.42),\n",
    "        'Jacksonville': (30.32, 81.70),\n",
    "        'Indianapolis': (39.78, 86.15),\n",
    "        'Austin': (30.27, 97.77),\n",
    "        'Columbus': (39.98, 82.98),\n",
    "        'Fort Worth': (32.75, 97.33),\n",
    "        'Charlotte': (35.23, 80.85),\n",
    "        'Memphis': (35.12, 89.97),\n",
    "        'Baltimore': (39.28, 76.62)\n",
    "    }\n",
    "\n",
    "    # initial state, a randomly-ordered itinerary\n",
    "    init_state = list(cities.keys())\n",
    "    random.shuffle(init_state)\n",
    "\n",
    "    # create a distance matrix\n",
    "    distance_matrix = {}\n",
    "    for ka, va in cities.items():\n",
    "        distance_matrix[ka] = {}\n",
    "        for kb, vb in cities.items():\n",
    "            if kb == ka:\n",
    "                distance_matrix[ka][kb] = 0.0\n",
    "            else:\n",
    "                distance_matrix[ka][kb] = distance(va, vb)\n",
    "\n",
    "    tsp = TravellingSalesmanProblem(init_state, distance_matrix)\n",
    "    tsp.steps = 100000\n",
    "    # since our state is just a list, slice is the fastest way to copy\n",
    "    tsp.copy_strategy = \"slice\"\n",
    "    state, e = tsp.anneal()\n",
    "\n",
    "    while state[0] != 'New York City':\n",
    "        state = state[1:] + state[:1]  # rotate NYC to start\n",
    "\n",
    "    print()\n",
    "    print(\"%i mile route:\" % e)\n",
    "    for city in state:\n",
    "        print(\"\\t\", city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move(self):\n",
    "    \"\"\"Swaps first element of two randomly chosen clusters.\"\"\"\n",
    "\n",
    "    #converts list of node lists into a 1D array of community labels\n",
    "    cluster_labels = []\n",
    "    label = 0\n",
    "    for cluster in self:\n",
    "        for elem in cluster:\n",
    "            cluster_labels.append(label)\n",
    "        label += 1\n",
    "\n",
    "#flatten self.state\n",
    "    flat_list = [item for sublist in self for item in sublist]\n",
    "    #sort cluster labels list by node label (not community label)\n",
    "    cluster_labels = [cluster_labels[flat_list.index(i)] for i in flat_list]\n",
    "    print(cluster_labels)\n",
    "    a = random.randint(0, len(cluster_labels)-1)\n",
    "    print(a)\n",
    "    change_node = cluster_labels[a] \n",
    "    #if current label is 0, change to 1 to increase hamming distance by 1\n",
    "    if change_node == 0:\n",
    "        cluster_labels[a] = 1\n",
    "#         #if current label is maximal label, -1 from it\n",
    "#         elif change_node == len(self.state):\n",
    "#             cluster_labels[a] = len(self.state)-1\n",
    "    #if current label is not 0 or maximal, increase or decrease by 1 with 0.5 prob\n",
    "    else:\n",
    "        updown = random.randint(0, 1)\n",
    "        if updown == 0:\n",
    "            cluster_labels[a] -= 1\n",
    "        cluster_labels[a] += 1\n",
    "    print(cluster_labels)\n",
    "    #convert back to list of lists\n",
    "    new_state = []\n",
    "    labels = set(cluster_labels)\n",
    "    for j in labels:\n",
    "        cluster = []\n",
    "        indices = [i for i, x in enumerate(cluster_labels) if x == j]\n",
    "        cluster.extend(list(np.array(flat_list)[indices]))\n",
    "        new_state.append(cluster)\n",
    "    return (new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 2]\n",
      "5\n",
      "[0, 0, 0, 1, 1, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 2, 4], [1, 3], [5]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move([[0, 2, 4], [1, 3], [5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
