{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Loading libraries ##################################################################################\n",
    "######################################################################################################\n",
    "import argparse, math, random, simanneal, sys\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from math import log\n",
    "from simanneal import Annealer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import hamming\n",
    "\n",
    "#####################################################################################################\n",
    "# Defining constants ################################################################################\n",
    "######################################################################################################\n",
    "TAU = 0.15 # Used in PageRank calculation (alpha is 1-tau where [RAB2009 says \\tau=0.15])\n",
    "PAGE_RANK = 'page_rank'\n",
    "MODULE_ID = 'module_id'\n",
    "\n",
    "#####################################################################################################\n",
    "# Auxiliary functions definition ####################################################################\n",
    "######################################################################################################\n",
    "## It's generally a good practice not to use numbers in functions' names \n",
    "##   (just pedantic CS rules that once were important because of old architectures).\n",
    "def log_base_two(probability):\n",
    "    \"Returns the log of probability in base 2\"\n",
    "    base_two_logarithm = log(probability,2)\n",
    "    return base_two_logarithm \n",
    "\n",
    "## Using only one 'return' statement is safer than branching them in conditional statements. \n",
    "##   It does not make a difference here, but in larger projects it makes code more readable \n",
    "##   (and 'friendlier' to collaborators).\n",
    "def partial_entropy(probability):\n",
    "    \"Half of the entropy function, as used in the InfoMap paper.\"\n",
    "    partial_entropy = 0\n",
    "    if probability != 0:\n",
    "        partial_entropy = probability * log2(probability)\n",
    "    return partial_entropy\n",
    "\n",
    "## Splitting this large function into 3 sub-functions. It's generally a good practice to split large \n",
    "##   functions into smaller ones: one function = one specific goal. In this case, having three \n",
    "##   functions makes sense because the smaller sub-routines could be used outside the scope of \n",
    "##   the main \"load and process\". As a rule of thumb: if you have around 20 or more lines in one \n",
    "##   function, you are probably doing too many things in the same block.\n",
    "def load_and_process_directed_graph(filename):\n",
    "    \"\"\"Loads the network from a file, normalizes it, and calculates its page rank \n",
    "        (storing the value in the network itself).\"\"\"\n",
    "    preProcessedGraph=load_directed_graph_from_file(filename)\n",
    "    normalizedGraph=normalize_edge_weights(preProcessedGraph)\n",
    "    pageRankedGraph=calculate_pagerank(normalizedGraph,TAU)\n",
    "    return pageRankedGraph\n",
    "\n",
    "### It is useful to define a function to this specific 'loading' task because you can use it outside\n",
    "###    the \"load and process\" scope to do other analyses and tests.\n",
    "def load_directed_graph_from_file(filename):\n",
    "    \"Imports a '.net' file and processes it as a directed network.\"\n",
    "    directedGraph = nx.DiGraph(nx.read_pajek(filename))\n",
    "    print(\n",
    "      \"Loaded a graph (%d nodes, %d edges)\" % \n",
    "      (len(directedGraph), len(directedGraph.edges()))\n",
    "    )\n",
    "    return directedGraph\n",
    "\n",
    "### Love this routine! So elegant.\n",
    "def normalize_edge_weights(directedGraph):\n",
    "    \"\"\"For each node in the network: calculates the total of the weights' values,\n",
    "        and then normalizes them to make their total equal to 1.\"\"\"\n",
    "    for node in directedGraph:\n",
    "        edges = directedGraph.edges(node, data=True)\n",
    "        total_weight = sum([data['weight'] for (_, _, data) in edges])\n",
    "        for (_, _, data) in edges:\n",
    "            data['weight'] = data['weight'] / total_weight\n",
    "    return directedGraph\n",
    "    \n",
    "### I think it's useful to have 'tau' as an argument, even when it is defined as a global constant.\n",
    "###   Even though the use of the global constant is completely justified in this case, having tau\n",
    "###   as an argument for your function makes it much more flexible; and the typing is a small price\n",
    "###   to pay for it.\n",
    "def calculate_pagerank(normalizedGraph, tau):\n",
    "    \"\"\"Get the network's PageRank.\"\"\"\n",
    "    page_ranks = nx.pagerank(normalizedGraph, alpha=1-TAU)\n",
    "    for (node, page_rank) in page_ranks.items():\n",
    "        normalizedGraph.node[node][PAGE_RANK] = page_rank\n",
    "    return normalizedGraph\n",
    "    \n",
    "## No changes. Just made the name a bit more specific.\n",
    "def load_coordinates_from_file(filename):\n",
    "    field_names = ['X','Y',\"w\"]\n",
    "    coords = pd.read_csv(filename, header=None, names=field_names)\n",
    "    coords = coords.loc[:,[\"X\",\"Y\"]]\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log base 2: True\n",
      "Partial entropy: True\n",
      "Loaded a graph (20 nodes, 400 edges)\n",
      "Loaded a graph (20 nodes, 400 edges)\n",
      "Loaded a graph (20 nodes, 400 edges)\n",
      "Partial Entropy Network: True\n",
      "Loaded coordinates: True\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################\n",
    "# 'Unit-testing' against original functions #########################################################\n",
    "#####################################################################################################\n",
    "testProbability=.2\n",
    "print(\"Log base 2: %r\" % (log_base_two(testProbability)==log_base_two(testProbability)))\n",
    "print(\"Partial entropy: %r\" % (partial_entropy(testProbability)==entropy1(testProbability)))\n",
    "\n",
    "graph=load_directed_graph_from_file(\"houses.net\")\n",
    "normalizedGraph=normalize_edge_weights(graph)\n",
    "calculate_pagerank(normalizedGraph,TAU)\n",
    "\n",
    "graphNew=load_and_process_directed_graph(\"houses.net\")\n",
    "partialEntropyNew=[partial_entropy(graphNew.node[node][PAGE_RANK]) for node in graph]\n",
    "graphOld=load_and_process_graph(\"houses.net\")\n",
    "partialEntropyOld=[entropy1(graphOld.node[node][PAGE_RANK]) for node in graph]\n",
    "print(\"Partial Entropy Network: %r\" % (partialEntropyNew==partialEntropyOld))\n",
    "\n",
    "loadedCoordinates=all(load_coordinates_from_file(\"coordinates.csv\")==load_coordinates(\"coordinates.csv\"))\n",
    "print(\"Loaded coordinates: %r\" % loadedCoordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Temperature        Energy    Accept   Improve     Elapsed   Remaining\n",
      "\r",
      " 25000.00000          9.07                         0:00:00            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a graph (20 nodes, 400 edges)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     2.50000          8.95   100.00%    34.00%     0:00:16     0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 mile route:\n",
      "\t [0, 0, 1, 1]\n",
      "\t [6]\n",
      "\t [16, 14, 13]\n",
      "\t [12, 8, 18, 3, 9, 17, 19, 4, 7, 2]\n",
      "\t [15, 5]\n"
     ]
    }
   ],
   "source": [
    "# class Module:\n",
    "#     \"\"\"Stores the information about a single module\"\"\"\n",
    "#     def __init__(self, module_id, nodes, graph):\n",
    "#         self.module_id = module_id\n",
    "#         self.nodes = frozenset(nodes)\n",
    "#         self.graph = graph\n",
    "#         self.prop_nodes = 1 - float(len(self.nodes)) / len(graph)\n",
    "#         # Set the module_id for every node\n",
    "# #         for node in nodes:\n",
    "# #             graph.node[node][MODULE_ID] = module_id\n",
    "#         # Compute the total PageRank\n",
    "#         self.total_pr = sum([graph.node[node][PAGE_RANK] for node in nodes])\n",
    "#         # Compute q_out, the exit probability of this module\n",
    "#         # .. Left half: tau * (n - n_i) / n * sum{alpha in i}(p_alpha)\n",
    "#         self.q_out = self.total_pr * TAU * self.prop_nodes\n",
    "#         # .. Right half: (1-tau) * sum{alpha in i}(sum{beta not in i}\n",
    "#         #                  p_alpha weight_alpha,beta)\n",
    "#         # This is what's in [RAB2009 eq. 6]. But it's apparently wrong if\n",
    "#         # node alpha has no out-edges, which is not in the paper.\n",
    "#         # ..\n",
    "#         # Implementing it with Seung-Hee's correction about dangling nodes\n",
    "#         for node in self.nodes:\n",
    "#             edges = graph.edges(node, data=True)\n",
    "#             page_rank = graph.node[node][PAGE_RANK]\n",
    "#             if len(edges) == 0:\n",
    "#                 self.q_out += page_rank * self.prop_nodes * (1 - TAU)\n",
    "#                 continue\n",
    "#             for (_, dest, data) in edges:\n",
    "#                 if dest not in self.nodes:\n",
    "#                     self.q_out += page_rank * data['weight'] * (1 - TAU)\n",
    "#         self.q_plus_p = self.q_out + self.total_pr\n",
    "\n",
    "#     def get_codebook_length(self):\n",
    "#         \"Computes module codebook length according to [RAB2009, eq. 3]\"\n",
    "#         first = -entropy1(self.q_out / self.q_plus_p)\n",
    "#         second = -sum( \\\n",
    "#                 [entropy1(self.graph.node[node][PAGE_RANK]/self.q_plus_p) \\\n",
    "#                     for node in self.nodes])\n",
    "#         return (self.q_plus_p) * (first + second)\n",
    "\n",
    "class GeoInfomap(Annealer):\n",
    "\n",
    "    \"\"\"Test annealer with a travelling salesman problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # pass extra data (the distance matrix) into the constructor\n",
    "    def __init__(self, state, module, graph, coordinates):\n",
    "        self.graph = graph\n",
    "        self.total_pr_entropy = sum([entropy1(graph.node[node][PAGE_RANK]) \\\n",
    "                for node in graph])\n",
    "#         self.module = [Module(module_id, mod, graph) \\\n",
    "#                 for (module_id, mod) in enumerate(state)]\n",
    "        d = 0\n",
    "        for mod in module:\n",
    "            for elem in range(len(mod)):\n",
    "                mod[elem] = int(mod[elem])    \n",
    "        for mod in module:\n",
    "            m = coordinates.loc[mod,]\n",
    "            d += np.mean(pairwise_distances(m, metric='euclidean'))\n",
    "        self.d = d \n",
    "\n",
    "        super(GeoInfomap, self).__init__(state)  # important!\n",
    "    def move(self):\n",
    "        #converts list of node lists into a 1D array of community labels\n",
    "        cluster_labels = []\n",
    "        label = 0\n",
    "        for cluster in self.state:\n",
    "            for elem in cluster:\n",
    "                cluster_labels.append(label)\n",
    "            label += 1\n",
    "\n",
    "    #flatten self.state\n",
    "        flat_list = [item for sublist in self.state for item in sublist]\n",
    "        #sort cluster labels list by node label (not community label)\n",
    "        cluster_labels = [cluster_labels[flat_list.index(i)] for i in flat_list]\n",
    "        a = random.randint(0, len(cluster_labels)-1)\n",
    "        change_node = cluster_labels[a] \n",
    "        #if current label is 0, change to 1 to increase hamming distance by 1\n",
    "        if change_node == 0:\n",
    "            cluster_labels[a] = 1\n",
    "        else:\n",
    "            updown = random.randint(0, 1)\n",
    "            if updown == 0:\n",
    "                cluster_labels[a] -= 1\n",
    "            cluster_labels[a] += 1\n",
    "\n",
    "        #convert back to list of lists\n",
    "        new_state = []\n",
    "        labels = set(cluster_labels)\n",
    "        for j in labels:\n",
    "            cluster = []\n",
    "            indices = [i for i, x in enumerate(cluster_labels) if x == j]\n",
    "            cluster.extend(list(np.array(flat_list)[indices]))\n",
    "            new_state.append(cluster)\n",
    "\n",
    "        self.state = new_state\n",
    "\n",
    "    def energy(self):\n",
    "        \"Compute the MDL of this clustering according to [RAB2009, eq. 4]\"\n",
    "        graph = self.graph\n",
    "        \n",
    "        total_qout = 0\n",
    "        total_qout_entropy = 0\n",
    "        total_both_entropy = 0\n",
    "        for mod in self.state:\n",
    "            nodes = frozenset(mod)\n",
    "            prop_nodes = 1 - float(len(nodes)) / len(graph)\n",
    "            total_pr = sum([graph.node[str(node)][PAGE_RANK] for node in nodes])\n",
    "            q_out = total_pr * TAU * prop_nodes\n",
    "\n",
    "            for node in mod:\n",
    "                edges = graph.edges(str(node), data=True)\n",
    "                page_rank = graph.node[str(node)][PAGE_RANK]\n",
    "                if len(edges) == 0:\n",
    "                    q_out += page_rank * prop_nodes * (1 - TAU)\n",
    "                    continue\n",
    "                for (_, dest, data) in edges:\n",
    "                    if dest not in self.state:\n",
    "                        q_out += page_rank * data['weight'] * (1 - TAU)\n",
    "                q_plus_p = q_out + total_pr\n",
    "        \n",
    "            q_out = q_out\n",
    "            total_qout += q_out\n",
    "            total_qout_entropy += entropy1(q_out)\n",
    "            total_both_entropy += entropy1(q_plus_p)\n",
    "        term1 = entropy1(total_qout)\n",
    "        term2 = -2 * total_qout_entropy\n",
    "        term3 = -self.total_pr_entropy\n",
    "        term4 = total_both_entropy\n",
    "        term5 = self.d\n",
    "        total =  term1 + term2 + term3 + term4 + 0.1*term5\n",
    "        #print(total)\n",
    "        return total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #networkX Digraph\n",
    "    graph = load_and_process_graph(\"houses.net\")#(options.graph_filename)\n",
    "\n",
    "    #coords is pandas dataframe of the coordinates\n",
    "    coords = load_coordinates(\"coordinates.csv\")\n",
    "    coords.index = np.arange(0, len(coords))\n",
    "\n",
    "    # single_nodes is the \"trivial\" module mapping\n",
    "    # initial module, as a list of lists\n",
    "    single_nodes = [[nodes] for nodes in graph]\n",
    "    single_nodes[0] = ['0','1']\n",
    "    single_nodes.remove(single_nodes[1]) \n",
    "    init_state = single_nodes\n",
    "\n",
    "    gi = GeoInfomap(state = init_state, module = init_state, graph = graph, coordinates = coords)\n",
    "    gi.steps = 10000\n",
    "    # since our state is just a list, slice is the fastest way to copy\n",
    "    gi.copy_strategy = \"slice\"\n",
    "    state, e = gi.anneal()\n",
    "        \n",
    "    print()\n",
    "   # print(state)\n",
    "    print(\"%i mile route:\" % e)\n",
    "    for city in state:\n",
    "        print(\"\\t\", city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Temperature        Energy    Accept   Improve     Elapsed   Remaining\n",
      "     2.50000       6898.57     6.00%     0.10%     0:00:01     0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6882 mile route:\n",
      "\t New York City\n",
      "\t Columbus\n",
      "\t Detroit\n",
      "\t Chicago\n",
      "\t Indianapolis\n",
      "\t Memphis\n",
      "\t Dallas\n",
      "\t Fort Worth\n",
      "\t Phoenix\n",
      "\t San Francisco\n",
      "\t San Jose\n",
      "\t Los Angeles\n",
      "\t San Diego\n",
      "\t San Antonio\n",
      "\t Austin\n",
      "\t Houston\n",
      "\t Jacksonville\n",
      "\t Charlotte\n",
      "\t Baltimore\n",
      "\t Philadelphia\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import random\n",
    "from simanneal import Annealer\n",
    "\n",
    "\n",
    "def distance(a, b):\n",
    "    \"\"\"Calculates distance between two latitude-longitude coordinates.\"\"\"\n",
    "    R = 3963  # radius of Earth (miles)\n",
    "    lat1, lon1 = math.radians(a[0]), math.radians(a[1])\n",
    "    lat2, lon2 = math.radians(b[0]), math.radians(b[1])\n",
    "    return math.acos(math.sin(lat1) * math.sin(lat2) +\n",
    "                     math.cos(lat1) * math.cos(lat2) * math.cos(lon1 - lon2)) * R\n",
    "\n",
    "\n",
    "class TravellingSalesmanProblem(Annealer):\n",
    "\n",
    "    \"\"\"Test annealer with a travelling salesman problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # pass extra data (the distance matrix) into the constructor\n",
    "    def __init__(self, state, distance_matrix):\n",
    "        self.distance_matrix = distance_matrix\n",
    "        super(TravellingSalesmanProblem, self).__init__(state)  # important!\n",
    "\n",
    "    def move(self):\n",
    "        \"\"\"Swaps two cities in the route.\"\"\"\n",
    "        a = random.randint(0, len(self.state) - 1)\n",
    "        b = random.randint(0, len(self.state) - 1)\n",
    "        self.state[a], self.state[b] = self.state[b], self.state[a]\n",
    "\n",
    "    def energy(self):\n",
    "        \"\"\"Calculates the length of the route.\"\"\"\n",
    "        e = 0\n",
    "        for i in range(len(self.state)):\n",
    "            e += self.distance_matrix[self.state[i-1]][self.state[i]]\n",
    "        return e\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # latitude and longitude for the twenty largest U.S. cities\n",
    "    cities = {\n",
    "        'New York City': (40.72, 74.00),\n",
    "        'Los Angeles': (34.05, 118.25),\n",
    "        'Chicago': (41.88, 87.63),\n",
    "        'Houston': (29.77, 95.38),\n",
    "        'Phoenix': (33.45, 112.07),\n",
    "        'Philadelphia': (39.95, 75.17),\n",
    "        'San Antonio': (29.53, 98.47),\n",
    "        'Dallas': (32.78, 96.80),\n",
    "        'San Diego': (32.78, 117.15),\n",
    "        'San Jose': (37.30, 121.87),\n",
    "        'Detroit': (42.33, 83.05),\n",
    "        'San Francisco': (37.78, 122.42),\n",
    "        'Jacksonville': (30.32, 81.70),\n",
    "        'Indianapolis': (39.78, 86.15),\n",
    "        'Austin': (30.27, 97.77),\n",
    "        'Columbus': (39.98, 82.98),\n",
    "        'Fort Worth': (32.75, 97.33),\n",
    "        'Charlotte': (35.23, 80.85),\n",
    "        'Memphis': (35.12, 89.97),\n",
    "        'Baltimore': (39.28, 76.62)\n",
    "    }\n",
    "\n",
    "    # initial state, a randomly-ordered itinerary\n",
    "    init_state = list(cities.keys())\n",
    "    random.shuffle(init_state)\n",
    "\n",
    "    # create a distance matrix\n",
    "    distance_matrix = {}\n",
    "    for ka, va in cities.items():\n",
    "        distance_matrix[ka] = {}\n",
    "        for kb, vb in cities.items():\n",
    "            if kb == ka:\n",
    "                distance_matrix[ka][kb] = 0.0\n",
    "            else:\n",
    "                distance_matrix[ka][kb] = distance(va, vb)\n",
    "\n",
    "    tsp = TravellingSalesmanProblem(init_state, distance_matrix)\n",
    "    tsp.steps = 100000\n",
    "    # since our state is just a list, slice is the fastest way to copy\n",
    "    tsp.copy_strategy = \"slice\"\n",
    "    state, e = tsp.anneal()\n",
    "\n",
    "    while state[0] != 'New York City':\n",
    "        state = state[1:] + state[:1]  # rotate NYC to start\n",
    "\n",
    "    print()\n",
    "    print(\"%i mile route:\" % e)\n",
    "    for city in state:\n",
    "        print(\"\\t\", city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move(self):\n",
    "    \"\"\"Swaps first element of two randomly chosen clusters.\"\"\"\n",
    "\n",
    "    #converts list of node lists into a 1D array of community labels\n",
    "    cluster_labels = []\n",
    "    label = 0\n",
    "    for cluster in self:\n",
    "        for elem in cluster:\n",
    "            cluster_labels.append(label)\n",
    "        label += 1\n",
    "\n",
    "#flatten self.state\n",
    "    flat_list = [item for sublist in self for item in sublist]\n",
    "    #sort cluster labels list by node label (not community label)\n",
    "    cluster_labels = [cluster_labels[flat_list.index(i)] for i in flat_list]\n",
    "    print(cluster_labels)\n",
    "    a = random.randint(0, len(cluster_labels)-1)\n",
    "    print(a)\n",
    "    change_node = cluster_labels[a] \n",
    "    #if current label is 0, change to 1 to increase hamming distance by 1\n",
    "    if change_node == 0:\n",
    "        cluster_labels[a] = 1\n",
    "#         #if current label is maximal label, -1 from it\n",
    "#         elif change_node == len(self.state):\n",
    "#             cluster_labels[a] = len(self.state)-1\n",
    "    #if current label is not 0 or maximal, increase or decrease by 1 with 0.5 prob\n",
    "    else:\n",
    "        updown = random.randint(0, 1)\n",
    "        if updown == 0:\n",
    "            cluster_labels[a] -= 1\n",
    "        cluster_labels[a] += 1\n",
    "    print(cluster_labels)\n",
    "    #convert back to list of lists\n",
    "    new_state = []\n",
    "    labels = set(cluster_labels)\n",
    "    for j in labels:\n",
    "        cluster = []\n",
    "        indices = [i for i, x in enumerate(cluster_labels) if x == j]\n",
    "        cluster.extend(list(np.array(flat_list)[indices]))\n",
    "        new_state.append(cluster)\n",
    "    return (new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 2]\n",
      "5\n",
      "[0, 0, 0, 1, 1, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 2, 4], [1, 3], [5]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move([[0, 2, 4], [1, 3], [5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MoNeT)",
   "language": "python",
   "name": "monet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
