{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'ArtificialLandscapes/data/Maps/R030_P050_C003_D010_CMgM.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-368e28b733a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ArtificialLandscapes/data/Maps/R030_P050_C003_D010_CMgM.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ArtificialLandscapes/data/Maps/R030_P050_C003_D010_CCrd.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnpmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_using\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'ArtificialLandscapes/data/Maps/R030_P050_C003_D010_CMgM.csv' does not exist"
     ]
    }
   ],
   "source": [
    "G = pd.read_csv('ArtificialLandscapes/data/Maps/R030_P050_C003_D010_CMgM.csv', header=None)\n",
    "coords = pd.read_csv('ArtificialLandscapes/data/Maps/R030_P050_C003_D010_CCrd.csv', header=None)\n",
    "\n",
    "npmat = np.matrix(G)\n",
    "G = nx.from_numpy_matrix(npmat, create_using = nx.DiGraph())\n",
    "#nx.write_pajek(G, \"ArtificialLandscapes/data/G1.net\")\n",
    "npmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Temperature        Energy    Accept   Improve     Elapsed   Remaining\n",
      " 22800.27098          7.46   100.00%    50.00%     0:00:00     0:00:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a graph (50 nodes, 801 edges)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     2.50000          7.42   100.00%    30.00%     0:00:02     0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t [16, 9, 2]\n",
      "\t [1, 4, 0, 11, 13, 12]\n",
      "\t [7, 15, 3, 10, 8, 17, 20, 18]\n",
      "\t [6, 34, 32, 25]\n",
      "\t [24, 19, 23, 22, 5, 26, 36]\n",
      "\t [14, 30, 21]\n",
      "\t [31, 41, 46]\n",
      "\t [29, 44, 48, 43, 35, 27, 47, 33]\n",
      "\t [38, 37, 42]\n",
      "\t [39, 28, 45]\n",
      "\t [40]\n",
      "\t [49]\n"
     ]
    }
   ],
   "source": [
    "TAU = 0.15\n",
    "PAGE_RANK = 'page_rank'\n",
    "MODULE_ID = 'module_id'\n",
    "\n",
    "def log2(prob):\n",
    "    \"Returns the log of prob in base 2\"\n",
    "    return log(prob, 2)\n",
    "\n",
    "def entropy1(prob):\n",
    "    \"\"\"Half of the entropy function, as used in the InfoMap paper.\n",
    "    entropy1(p) = p * log2(p)\n",
    "    \"\"\"\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    return prob * log2(prob)\n",
    "\n",
    "def load_and_process_graph(filename):\n",
    "    \"\"\"Load the graph, normalize edge weights, compute pagerank, and store all\n",
    "    this back in node data.\"\"\"\n",
    "    # Load the graph\n",
    "    graph = nx.DiGraph(nx.read_pajek(filename))\n",
    "    #graph = nx.read_pajek(filename)\n",
    "    print (\"Loaded a graph (%d nodes, %d edges)\" % (len(graph),\n",
    "            len(graph.edges())))\n",
    "    # Compute the normalized edge weights\n",
    "    for node in graph:\n",
    "        edges = graph.edges(node, data=True)\n",
    "        total_weight = sum([data['weight'] for (_, _, data) in edges])\n",
    "        for (_, _, data) in edges:\n",
    "            data['weight'] = data['weight'] / total_weight\n",
    "    # Get its PageRank, alpha is 1-tau where [RAB2009 says \\tau=0.15]\n",
    "    page_ranks = nx.pagerank(graph, alpha=1-TAU)\n",
    "    for (node, page_rank) in page_ranks.items():\n",
    "        graph.node[node][PAGE_RANK] = page_rank\n",
    "    return graph\n",
    "\n",
    "def load_coordinates(filename):\n",
    "    field_names = ['X', 'Y', \"w\"]\n",
    "    coords = pd.read_csv(filename, header=None, names=field_names)\n",
    "    coords = coords.loc[:,[\"X\",\"Y\"]]\n",
    "    return coords\n",
    "\n",
    "class GeoInfomap(Annealer):\n",
    "\n",
    "    \"\"\"Test annealer with a travelling salesman problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # pass extra data (the distance matrix) into the constructor\n",
    "    def __init__(self, state, module, graph, coordinates):\n",
    "        self.graph = graph\n",
    "        self.total_pr_entropy = sum([entropy1(graph.node[node][PAGE_RANK]) \\\n",
    "                for node in graph])\n",
    "        d = 0\n",
    "        for mod in module:\n",
    "            for elem in range(len(mod)):\n",
    "                mod[elem] = int(mod[elem])    \n",
    "        for mod in module:\n",
    "            m = coordinates.loc[mod,]\n",
    "            d += np.mean(pairwise_distances(m, metric='euclidean'))\n",
    "        self.d = d \n",
    "\n",
    "        super(GeoInfomap, self).__init__(state)  # important!\n",
    "    def move(self):\n",
    "        #converts list of node lists into a 1D array of community labels\n",
    "        cluster_labels = []\n",
    "        label = 0\n",
    "        for cluster in self.state:\n",
    "            for elem in cluster:\n",
    "                cluster_labels.append(label)\n",
    "            label += 1\n",
    "\n",
    "    #flatten self.state\n",
    "        flat_list = [item for sublist in self.state for item in sublist]\n",
    "        #sort cluster labels list by node label (not community label)\n",
    "        cluster_labels = [cluster_labels[flat_list.index(i)] for i in flat_list]\n",
    "        a = random.randint(0, len(cluster_labels)-1)\n",
    "        change_node = cluster_labels[a] \n",
    "        #if current label is 0, change to 1 to increase hamming distance by 1\n",
    "        if change_node == 0:\n",
    "            cluster_labels[a] = 1\n",
    "        else:\n",
    "            updown = random.randint(0, 1)\n",
    "            if updown == 0:\n",
    "                cluster_labels[a] -= 1\n",
    "            cluster_labels[a] += 1\n",
    "\n",
    "        #convert back to list of lists\n",
    "        new_state = []\n",
    "        labels = set(cluster_labels)\n",
    "        for j in labels:\n",
    "            cluster = []\n",
    "            indices = [i for i, x in enumerate(cluster_labels) if x == j]\n",
    "            cluster.extend(list(np.array(flat_list)[indices]))\n",
    "            new_state.append(cluster)\n",
    "\n",
    "        self.state = new_state\n",
    "\n",
    "    def energy(self):\n",
    "        \"Compute the MDL of this clustering according to [RAB2009, eq. 4]\"\n",
    "        graph = self.graph\n",
    "        \n",
    "        total_qout = 0\n",
    "        total_qout_entropy = 0\n",
    "        total_both_entropy = 0\n",
    "        for mod in self.state:\n",
    "            nodes = frozenset(mod)\n",
    "            prop_nodes = 1 - float(len(nodes)) / len(graph)\n",
    "            total_pr = sum([graph.node[str(node)][PAGE_RANK] for node in nodes])\n",
    "            q_out = total_pr * TAU * prop_nodes\n",
    "\n",
    "            for node in mod:\n",
    "                edges = graph.edges(str(node), data=True)\n",
    "                page_rank = graph.node[str(node)][PAGE_RANK]\n",
    "                if len(edges) == 0:\n",
    "                    q_out += page_rank * prop_nodes * (1 - TAU)\n",
    "                    continue\n",
    "                for (_, dest, data) in edges:\n",
    "                    if dest not in self.state:\n",
    "                        q_out += page_rank * data['weight'] * (1 - TAU)\n",
    "                q_plus_p = q_out + total_pr\n",
    "        \n",
    "            q_out = q_out\n",
    "            total_qout += q_out\n",
    "            total_qout_entropy += entropy1(q_out)\n",
    "            total_both_entropy += entropy1(q_plus_p)\n",
    "        term1 = entropy1(total_qout)\n",
    "        term2 = -2 * total_qout_entropy\n",
    "        term3 = -self.total_pr_entropy\n",
    "        term4 = total_both_entropy\n",
    "        term5 = self.d\n",
    "        total =  term1 + term2 + term3 + term4 + term5\n",
    "        #print(total)\n",
    "        return total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #networkX Digraph\n",
    "    graph = load_and_process_graph(\"G1.net\")#(options.graph_filename)\n",
    "\n",
    "    #coords is pandas dataframe of the coordinates\n",
    "    coords = load_coordinates(\"ArtificialLandscapes/data/unzipForMaps/R030_P050_C003_D010_CCrd.csv\")\n",
    "    coords.index = np.arange(0, len(coords))\n",
    "\n",
    "    # single_nodes is the \"trivial\" module mapping\n",
    "    # initial module, as a list of lists\n",
    "    single_nodes = [[nodes] for nodes in graph]\n",
    "    init_state = single_nodes\n",
    "\n",
    "    gi = GeoInfomap(state = init_state, module = init_state, graph = graph, coordinates = coords)\n",
    "    gi.steps = 1000\n",
    "    # since our state is just a list, slice is the fastest way to copy\n",
    "    gi.copy_strategy = \"slice\"\n",
    "    state, e = gi.anneal()\n",
    "        \n",
    "    print()\n",
    "   # print(state)\n",
    "    #print(\"%i mile route:\" % e)\n",
    "    for city in state:\n",
    "        print(\"\\t\", city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  6,  6,  6,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  9,  9,  9, 10, 11])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = np.array(1)\n",
    "i = 0\n",
    "for j in state:\n",
    "    cluster = np.append(cluster, np.array([i for elem in j]))\n",
    "    i += 1\n",
    "cluster = cluster.flatten()[1:]\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coords['cluster'] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'co'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"coords\"[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
