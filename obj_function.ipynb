{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a graph (20 nodes, 400 edges)\n",
      "This clustering has MDL 178.75 (Index 0.51, Module 4.01)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"A simple MDL calculator using the Map Equation and a graph.\"\"\"\n",
    "\n",
    "import argparse\n",
    "from math import log\n",
    "import networkx as nx\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "TAU = 0.15\n",
    "PAGE_RANK = 'page_rank'\n",
    "MODULE_ID = 'module_id'\n",
    "\n",
    "def log2(prob):\n",
    "    \"Returns the log of prob in base 2\"\n",
    "    return log(prob, 2)\n",
    "\n",
    "def entropy1(prob):\n",
    "    \"\"\"Half of the entropy function, as used in the InfoMap paper.\n",
    "    entropy1(p) = p * log2(p)\n",
    "    \"\"\"\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    return prob * log2(prob)\n",
    "\n",
    "class Module:\n",
    "    \"\"\"Stores the information about a single module\"\"\"\n",
    "    def __init__(self, module_id, nodes, graph):\n",
    "        self.module_id = module_id\n",
    "        self.nodes = frozenset(nodes)\n",
    "        self.graph = graph\n",
    "        self.prop_nodes = 1 - float(len(self.nodes)) / len(graph)\n",
    "        # Set the module_id for every node\n",
    "        for node in nodes:\n",
    "            graph.node[node][MODULE_ID] = module_id\n",
    "        # Compute the total PageRank\n",
    "        self.total_pr = sum([graph.node[node][PAGE_RANK] for node in nodes])\n",
    "        # Compute q_out, the exit probability of this module\n",
    "        # .. Left half: tau * (n - n_i) / n * sum{alpha in i}(p_alpha)\n",
    "        self.q_out = self.total_pr * TAU * self.prop_nodes\n",
    "        # .. Right half: (1-tau) * sum{alpha in i}(sum{beta not in i}\n",
    "        #                  p_alpha weight_alpha,beta)\n",
    "        # This is what's in [RAB2009 eq. 6]. But it's apparently wrong if\n",
    "        # node alpha has no out-edges, which is not in the paper.\n",
    "        # ..\n",
    "        # Implementing it with Seung-Hee's correction about dangling nodes\n",
    "        for node in self.nodes:\n",
    "            edges = graph.edges(node, data=True)\n",
    "            page_rank = graph.node[node][PAGE_RANK]\n",
    "            if len(edges) == 0:\n",
    "                self.q_out += page_rank * self.prop_nodes * (1 - TAU)\n",
    "                continue\n",
    "            for (_, dest, data) in edges:\n",
    "                if dest not in self.nodes:\n",
    "                    self.q_out += page_rank * data['weight'] * (1 - TAU)\n",
    "        self.q_plus_p = self.q_out + self.total_pr\n",
    "\n",
    "    def get_codebook_length(self):\n",
    "        \"Computes module codebook length according to [RAB2009, eq. 3]\"\n",
    "        first = -entropy1(self.q_out / self.q_plus_p)\n",
    "        second = -sum( \\\n",
    "                [entropy1(self.graph.node[node][PAGE_RANK]/self.q_plus_p) \\\n",
    "                    for node in self.nodes])\n",
    "        return (self.q_plus_p) * (first + second)\n",
    "\n",
    "\n",
    "class Clustering:\n",
    "    \"Stores a clustering of the graph into modules\"\n",
    "    def __init__(self, graph, modules, coordinates):\n",
    "        self.graph = graph\n",
    "        self.total_pr_entropy = sum([entropy1(graph.node[node][PAGE_RANK]) \\\n",
    "                for node in graph])\n",
    "        self.modules = [Module(module_id, module, graph) \\\n",
    "                for (module_id, module) in enumerate(modules)]\n",
    "        self.coordinates = coordinates\n",
    "        \n",
    "        d = 0\n",
    "        for mod in modules:\n",
    "            for elem in range(len(mod)):\n",
    "                mod[elem] = int(mod[elem])    \n",
    "        for mod in modules:\n",
    "            m = coords.loc[mod,]\n",
    "            d += np.mean(pairwise_distances(m, metric='euclidean'))\n",
    "        self.d = d \n",
    "        \n",
    "    def get_mdl(self):\n",
    "        \"Compute the MDL of this clustering according to [RAB2009, eq. 4]\"\n",
    "        total_qout = 0\n",
    "        total_qout_entropy = 0\n",
    "        total_both_entropy = 0\n",
    "        for mod in self.modules:\n",
    "            q_out = mod.q_out\n",
    "            total_qout += q_out\n",
    "            total_qout_entropy += entropy1(q_out)\n",
    "            total_both_entropy += entropy1(mod.q_plus_p)\n",
    "        term1 = entropy1(total_qout)\n",
    "        term2 = -2 * total_qout_entropy\n",
    "        term3 = -self.total_pr_entropy\n",
    "        term4 = total_both_entropy\n",
    "        term5 = self.d\n",
    "        \n",
    "        return term1 + term2 + term3 + term4 + term5\n",
    "\n",
    "    def get_index_codelength(self):\n",
    "        \"Compute the index codebook length according to [RAB2009, eq. 2]\"\n",
    "        if len(self.modules) == 1:\n",
    "            return 0\n",
    "        total_q = sum([mod.q_out for mod in self.modules])\n",
    "        entropy = -sum([entropy1(mod.q_out / total_q) for mod in self.modules])\n",
    "        return total_q * entropy\n",
    "\n",
    "    def get_module_codelength(self):\n",
    "        \"Compute the module codebook length according to [RAB2009, eq. 3]\"\n",
    "        return sum([mod.get_codebook_length() for mod in self.modules])\n",
    "\n",
    "def print_tree_file(graph, modules):\n",
    "    \"\"\"Produces a .tree file from the given clustering that is compatible with\n",
    "    the InfoMapCheck utility.\"\"\"\n",
    "    for (mod_id, mod) in enumerate(modules):\n",
    "        for (node_id, node) in enumerate(mod):\n",
    "            print (\"%d:%d %f \\\"%s\\\"\" % (mod_id+1, node_id+1,\n",
    "                    graph.node[node][PAGE_RANK], node))\n",
    "\n",
    "def load_and_process_graph(filename):\n",
    "    \"\"\"Load the graph, normalize edge weights, compute pagerank, and store all\n",
    "    this back in node data.\"\"\"\n",
    "    # Load the graph\n",
    "    graph = nx.DiGraph(nx.read_pajek(filename))\n",
    "    #graph = nx.read_pajek(filename)\n",
    "    print (\"Loaded a graph (%d nodes, %d edges)\" % (len(graph),\n",
    "            len(graph.edges())))\n",
    "    # Compute the normalized edge weights\n",
    "    for node in graph:\n",
    "        edges = graph.edges(node, data=True)\n",
    "        total_weight = sum([data['weight'] for (_, _, data) in edges])\n",
    "        for (_, _, data) in edges:\n",
    "            data['weight'] = data['weight'] / total_weight\n",
    "    # Get its PageRank, alpha is 1-tau where [RAB2009 says \\tau=0.15]\n",
    "    page_ranks = nx.pagerank(graph, alpha=1-TAU)\n",
    "    for (node, page_rank) in page_ranks.items():\n",
    "        graph.node[node][PAGE_RANK] = page_rank\n",
    "    return graph\n",
    "\n",
    "def load_coordinates(filename):\n",
    "    field_names = ['X', 'Y', \"w\"]\n",
    "    coords = pd.read_csv(filename, header=None, names=field_names)\n",
    "    coords = coords.loc[:,[\"X\",\"Y\"]]\n",
    "    #coords = coords.as_matrix()\n",
    "    return coords\n",
    "\n",
    "def main(argv):\n",
    "    \"Read the supplied graph and modules and output MDL\"\n",
    "    # Read the arguments\n",
    "# parser = argparse.ArgumentParser(description=\"Calculate the infomap\")\n",
    "# parser.add_argument('-g', '--graph-filename', type=argparse.FileType('r'),\n",
    "#         help=\"the .net file to use as the graph\", required=True)\n",
    "# parser.add_argument('-m', '--module-filename', default=\"2009_figure3a.mod\",\n",
    "#         help=\"the .mod file to use as the clustering\")\n",
    "# options = parser.parse_args(argv[1:])\n",
    "\n",
    "#networkX Digraph\n",
    "graph = load_and_process_graph(\"houses.net\")#(options.graph_filename)\n",
    "\n",
    "#coords is pandas dataframe of the coordinates\n",
    "coords = load_coordinates(\"coordinates.csv\")\n",
    "coords.index = np.arange(0, len(coords))\n",
    "\n",
    "# single_nodes is the \"trivial\" module mapping\n",
    "single_nodes = [[nodes] for nodes in graph]\n",
    "\n",
    "# If clustering provided, use it.\n",
    "try:\n",
    "    modules = [line.strip().split() for line in open(\"houses.mod\")]\n",
    "except IOError:\n",
    "    print (\">>\", sys.exc_info()[0])\n",
    "    print (\">> No .mod file provided, or error reading it\")\n",
    "    print (\">> Using default clustering of every node in its own module\")\n",
    "    modules = single_nodes\n",
    "\n",
    "clustering = Clustering(graph, modules, coords)\n",
    "print (\"This clustering has MDL %.2f (Index %.2f, Module %.2f)\" % \\\n",
    "    (clustering.get_mdl(), clustering.get_index_codelength(),\n",
    "            clustering.get_module_codelength()))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(sys.argv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
